```{r}
# load in dataset
library(ggplot2)
library(tidyr)
library(caret)
library(stats)
library(factoextra)
library(e1071)
library(pROC)
library(xgboost)
df <- read.csv("C:/MIDS/ADS-503_Applied_Predictive_Modeling/ADS_503_team_2_final_project/data/breast_cancer_FNA_data.csv")
# remove the x and Id from the end of the data set since it is all null values
# "x" column is an error from csv column spacing, and can be removed entirely. 
df$X <- NULL
df$id <- NULL
df_diag <- df
```

##### Summary Stats, Dimensions & NA values

```{r}
# get descriptive statistics for the data set
summary(df)
```

```{r}
# data set dimensions
cat("Dimensions of dataset:", dim(df))
```

```{r}
# NA values
df_na_counts <- sum(is.na(df))
cat("NA Sum:", df_na_counts)
```

#### Distribution of Outcomes

```{r}
# Calculate percentages
percentage_M <- sum(df$diagnosis == "M") / nrow(df) * 100
percentage_B <- sum(df$diagnosis == "B") / nrow(df) * 100

# Print the percentages
cat("Percentage of Malignant diagnosis:", percentage_M,"%\n")
cat("Percentage of Benign diagnosis:", percentage_B,"%\n")
```

#### Exploring Possible Near Zero Variances

```{r}
degeneratecols <- nearZeroVar(df)
degeneratecols
```

There appears to be no degenerate variables.

#### Splitting into groups to allow for easier visualizations

```{r}
# Identify the columns containing "mean"
mean_columns <- grep("mean", names(df), value = TRUE)

# Identify the columns containing "se"
se_columns <- grep("se", names(df), value = TRUE)

# Identify the columns containing "worst"
worst_columns <- grep("worst", names(df), value = TRUE)

# Split the dataframe into three groups based on the keywords
df_mean <- df[, mean_columns]
df_se <- df[, se_columns]
df_worst <- df[, worst_columns]
```

```{r}

# Prepare the dataframe 
df_clean <- df[, !(names(df) %in% c("X", "id"))]
df_clean
variable_names <- colnames(df_clean)
print(variable_names)

# Create empty data frames
df_mean <- data.frame(diagnosis = df_clean$diagnosis)
df_se <- data.frame(diagnosis = df_clean$diagnosis)
df_worst <- data.frame(diagnosis = df_clean$diagnosis)

# Loop over variable names
for (variable_name in variable_names) {
  # Extract the part after the "_" delimiter
  variable_type <- sub(".+_([^_]+)$", "\\1", variable_name)
  # Assign the column to the appropriate data frame based on variable type
  if (variable_type == "mean") {
    df_mean[[variable_name]] <- df_clean[[variable_name]]
  } else if (variable_type == "se") {
    df_se[[variable_name]] <- df_clean[[variable_name]]
  } else if (variable_type == "worst") {
    df_worst[[variable_name]] <- df_clean[[variable_name]]
  }
}

### Analyze the Feature Density & Correlation between variables
# Visualization for the "Mean" Variables
ggpairs(df_mean, aes(color=diagnosis, alpha=0.75), lower=list(continuous="smooth"))+ theme_bw()+
  labs(title="Feature Density & Correlation - Cancer Means")+theme(plot.title=element_text(face='bold',color='black',hjust=0.5,size=20))
# Visualization for the "Standard Error" Variables
ggpairs(df_se, aes(color=diagnosis, alpha=0.75), lower=list(continuous="smooth"))+ theme_bw()+
  labs(title="Feature Density & Correlation - Cancer Standard Error")+theme(plot.title=element_text(face='bold',color='black',hjust=0.5,size=20))
# Visualization for the "Worst" Variables
ggpairs(df_worst, aes(color=diagnosis, alpha=0.75), lower=list(continuous="smooth"))+ theme_bw()+
  labs(title="Feature Density & Correlation - Cancer Worst (Mean of the three largest values)")+theme(plot.title=element_text(face='bold',color='black',hjust=0.5,size=20))

```

```{r}
# Pearson Correlation of all features
df_clean$diagnosis <- as.integer(factor(df_clean$diagnosis))-1
correlations <- cor(df_clean,method="pearson")
corrplot(correlations, number.cex = .9, method = "square", 
         hclust.method = "ward", order = "FPC",
         type = "full", tl.cex=0.7,tl.col = "black")
```
```{r}
# Calculate the VIF values (using linear model) for the predictor variables to look for multicollinearity
# VIF below 5 are generally considered acceptable, 5-10 suggest moderate multicollinearity, and above 10 show high multicollinearity
lm_model <- lm(diagnosis ~ ., data = df_clean)
vif_values <- vif(lm_model)
vif_table <- data.frame(Variable = names(vif_values), VIF = vif_values)
vif_table <- vif_table[order(vif_table$VIF), ]
vif_table$VIF <- round(vif_table$VIF)
y_axis_limit <- 4000
vif_plot <- ggplot(vif_table, aes(x = reorder(Variable, VIF), y = VIF)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  geom_text(aes(label = VIF), vjust = -0.5) +  # Add integer value labels
  labs(x = "Variable", y = "Variance Inflation Factor (VIF)") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ggtitle("VIF Values of Predictor Variables (Ascending Order)") +
  ylim(0, y_axis_limit)  # Set the y-axis limits
print(vif_plot)
```
```{r}
### Calculate the tolerance (reciprocal of VIF) for the predictor variables to look for multicollinearity 
# Generally, a tolerance value less than 0.1 or 0.2 is often considered indicative of multicollinearity.
tolerance <- 1 / vif_values
tolerance_table <- data.frame(Variable = names(tolerance), Tolerance = tolerance)
tolerance_table <- tolerance_table[order(tolerance_table$Tolerance), ]
tolerance_table$Tolerance <- round(tolerance_table$Tolerance, 2)
y_axis_limit <- 0.27
tolerance_plot <- ggplot(tolerance_table, aes(x = reorder(Variable, Tolerance), y = Tolerance)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  geom_text(aes(label = Tolerance), vjust = -0.5, hjust = 0.5, color = "black", size = 3) +  # Add value labels
  labs(x = "Variable", y = "Tolerance") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ggtitle("Tolerance Values of Predictor Variables (Ascending Order)") +
  ylim(0, y_axis_limit)  # Set the y-axis limits
tolerance_plot
```


```{r}
# Calculate the eigenvalues for the predictor variables to look for multicollinearity 
# Small eigenvalues suggest potential multicollinearity issues
correlations <- cor(df_clean[-1], method = "pearson")
eigenvalues <- eigen(correlations)$values
eigen_table <- data.frame(Variable = colnames(correlations), Eigenvalue = eigenvalues)
eigen_table <- eigen_table[eigen_table$Variable != "diagnosis", ]
eigen_table <- eigen_table[order(eigen_table$Eigenvalue), ]
# Round the Eigenvalue values for better aesthetics
eigen_table$Eigenvalue <- round(eigen_table$Eigenvalue, 2)
y_axis_limit <- 14
eigen_plot <- ggplot(eigen_table, aes(x = reorder(Variable, Eigenvalue), y = Eigenvalue)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  geom_text(aes(label = Eigenvalue), vjust = -0.5, hjust = 0.5, color = "black", size = 3) +  # Add value labels
  labs(x = "Variable", y = "Eigenvalue") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ggtitle("Eigenvalues of Variables (Ascending Order)") +
  ylim(0, y_axis_limit)  # Set the y-axis limits
eigen_plot

```
```{r}
### Principal Components Analysis (PCA) transform
var_only <- df_clean[, !(names(df_clean) %in% c("diagnosis"))] # predictor variables only
diagnosis <- raw.data[, 2] # target variable only 
var_pca <- prcomp(var_only, center = TRUE, scale. = TRUE)
fviz_eig(var_pca, addlabels = TRUE, ylim = c(0, 50),
         title = "Principal Components Analysis (PCA)",
         subtitle = NULL,
         xlab = "Principal Component",
         ylab = "Proportion of Variance Explained",
         geom = "line",
         linecolor = "red",
         pointsize = 2,
         pointshape = 21,
         pointfill = "white",
         pointcolor = "red",
         legend.title = "Principal Components",
         legend.position = "right")
# Adjust plot margins
par(mar = c(5, 5, 4, 2) + 0.1)
```
```{r}
# Create PCA biplot with customized aesthetics
pca_biplot <- fviz_pca_biplot(var_pca,
                          geom.ind = "point",
                          col.ind = diagnosis,
                          palette = c("blue", "red"),
                          addEllipses = TRUE,
                          axes.linetype = "dashed",
                          title = "Principal Components Analysis (PCA)",
                          xlab = "PC1 (Proportion of Variance Explained)",
                          ylab = "PC2 (Proportion of Variance Explained)",
                          legend.title = "Diagnosis",
                          legend.position = "right",
                          legend.shape = "circle",
                          legend.label = c("Benign", "Malignant"))
pca_biplot

```

```{r}
### t-SNE transform for dimensionality reduction
colors <- c("red", "blue")
names(colors) = unique(diagnosis)
set.seed(0)
tsne <- Rtsne(var_only, dims=2, perplexity=30, 
              verbose=TRUE, pca=TRUE, 
              theta=0.01, max_iter=1000)
plot(tsne$Y, t='n', main="t-Distributed Stochastic Neighbor Embedding (t-SNE)",
     xlab="t-SNE 1st dimm.", ylab="t-SNE 2nd dimm.")
text(tsne$Y, labels=diagnosis, cex=0.5, col=colors[diagnosis])
```


#### df_var data frame for visualizing just predictors

```{r}
remove <- c("diagnosis")
df_var <- df[, !(colnames(df) %in% remove)]
```

#### Histograms of all predictor variables

```{r}
par(mfrow = c(5, 2))
par(mar = c(2.5, 2, 1, 1))
par(cex.main = 1)

for (i in 1:ncol(df_mean)) {
  hist(df_var[, i], xlab = names(df_var)[i], main = paste(names(df_var)[i]))
}

for (i in 1:ncol(df_se)) {
  hist(df_var[, i], xlab = names(df_var)[i], main = paste(names(df_var)[i]))
}

for (i in 1:ncol(df_worst)) {
  hist(df_var[, i], xlab = names(df_var)[i], main = paste(names(df_var)[i]))
}

```

```{r}
# build a boxplot to identity the outliers that were produced in the summary statistics
# split the data into groups of 5 and create 6 graphs for the plots since there are 30 features and 1 target
# data will be split into variables bx_1, bx_2, bx_3, bx_4, bx_5 for boxplot purposes
colors <- c('red', 'blue')
bx_1 <- df[, 2:6]
bx_2 <- df[, 7:12]
bx_3 <- df[, 13:18]
bx_4 <- df[, 19:24]
bx_5 <- df[, 25:31]
```

All of these predictors have outliers however the predictor with the most are texture_mean, area_mean ad smoothness mean.

For bx_2 all of these also have outliers and I would say they all have some pretty significant outliers.

```{r}
# Create a data frame for plotting
plot_data_1 <- data.frame(
  value = c(bx_1$radius_mean, bx_1$texture_mean, bx_1$perimeter_mean, bx_1$smoothness_mean, bx_1$area_mean),
  diagnosis = rep(df$diagnosis, 5),
  feature = rep(c("Radius Mean", "Texture Mean", "Perimeter Mean", "Smoothness Mean", "Area Mean"), each = nrow(df))
)

# Create the boxplot using ggplot2
ggplot(plot_data_1, aes(x = diagnosis, y = value, fill = diagnosis)) +
  geom_boxplot() +
  facet_wrap(~ feature, scales = "free_y", nrow = 1) +
  scale_fill_manual(values = colors, labels = c('Bengin', 'Malignant')) +
  xlab("Diagnosis") +
  ylab("Value") +
  theme(legend.position = "top")
```

```{r}
# Create a data frame for plotting
plot_data_2 <- data.frame(
  value = c(bx_2$fractal_dimension_mean,bx_2$symmetry_mean, bx_2$radius_se, bx_2$concave.points_mean, bx_2$concavity_mean,bx_2$compactness_mean  ),
  diagnosis = rep(df$diagnosis, 6),
  feature = rep(c("FractalMean", "Sym Mean", "Radius Se", "Con Mean", "Concavity Mean", 'Compact Mean'), each = nrow(df))
)

# Create the boxplot using ggplot2
ggplot(plot_data_2, aes(x = diagnosis, y = value, fill = diagnosis)) +
  geom_boxplot() +
  facet_wrap(~ feature, scales = "free_y", nrow = 1) +
  scale_fill_manual(values = colors, labels = c('Bengin', 'Malignant')) +
  xlab("Diagnosis") +
  ylab("Value") +
  theme(legend.position = "top")
```

```{r}
# Create a data frame for plotting
plot_data_3 <- data.frame(
  value = c(bx_3$texture_se,bx_3$perimeter_se, bx_3$area_se, bx_3$smoothness_se, bx_3$compactness_se, bx_3$concavity_se),
  diagnosis = rep(df$diagnosis, 6),
  feature = rep(c("Texture Se", "Perimeter Se", "Area Se", "Smoothness Se", "Compact Se", 'Concave Se'), each = nrow(df))
)

# Create the boxplot using ggplot2
ggplot(plot_data_3, aes(x = diagnosis, y = value, fill = diagnosis)) +
  geom_boxplot() +
  facet_wrap(~ feature, scales = "free_y", nrow = 1) +
  scale_fill_manual(values = colors, labels = c('Bengin', 'Malignant')) +
  xlab("Diagnosis") +
  ylab("Value") +
  theme(legend.position = "top")

```

```{r}

# Create a data frame for plotting
plot_data_4 <- data.frame(
  value = c(bx_4$concave.points_se,bx_4$symmetry_se, bx_4$fractal_dimension_se, bx_4$radius_worst, bx_4$texture_worst, bx_4$perimeter_worst),
  diagnosis = rep(df$diagnosis, 6),
  feature = rep(c("Concave Se", "Symmetry Se", "Fractal Se", "Radius Worst", "Texture Worst", 'Perimeter Worst'), each = nrow(df))
)

# Create the boxplot 
ggplot(plot_data_4, aes(x = diagnosis, y = value, fill = diagnosis)) +
  geom_boxplot() +
  facet_wrap(~ feature, scales = "free_y", nrow = 1) +
  scale_fill_manual(values = colors, labels = c('Bengin', 'Malignant')) +
  xlab("Diagnosis") +
  ylab("Value") +
  theme(legend.position = "top")

```

```{r}
# Create a data frame for plotting
plot_data_5 <- data.frame(
  value = c(bx_5$area_worst ,bx_5$smoothness_worst, bx_5$compactness_worst, bx_5$concave.points_worst, bx_5$symmetry_worst, bx_5$fractal_dimension_worst),
  diagnosis = rep(df$diagnosis, 6),
  feature = rep(c("Area Worst", "Smoothness Worst", "Compact Worst", "Concave Worst", "symmetry Worst", 'Fractal Worst'), each = nrow(df))
)

# Create the boxplot 
ggplot(plot_data_5, aes(x = diagnosis, y = value, fill = diagnosis)) +
  geom_boxplot() +
  facet_wrap(~ feature, scales = "free_y", nrow = 1) +
  scale_fill_manual(values = colors, labels = c('Bengin', 'Malignant')) +
  xlab("Diagnosis") +
  ylab("Value") +
  theme(legend.position = "top")
```

#### Skew

```{r pre_trans_skew}
# List of group names
group_names <- c("mean", "se", "worst")

# Set the main title font size
par(cex.main = 0.8)

# Loop through each group
for (group_name in group_names) {
  # Subset the columns based on group name
  group_columns <- grep(group_name, names(df_var), value = TRUE)

  # Calculate skewness and create plots for each column in the group
  num_plots <- length(group_columns)
  num_rows <- ceiling(num_plots / 5)
  num_cols <- min(num_plots, 5)
  
  # Set the plotting layout for the current group
  par(mfrow = c(num_rows, num_cols))
  
  # Loop through each column in the group
  for (i in 1:num_plots) {
    col_name <- group_columns[i]
    skewness <- skewness(df_var[[col_name]])
    
    # Create a histogram to visualize skewness
    hist(df_var[[col_name]], main = col_name,
         xlab = "Values", ylab = "Frequency",
         col = "gray", border = "black")
    
    # Add skewness value to the plot
    mtext(paste("Skew:", round(skewness, 2)), side = 3, line = -.25, cex = 0.5, font = 1)
  }
  
  # Reset the plotting layout
  par(mfrow = c(1, 1))
}
```

#### Skew in dataframe format for easy reference

```{r pre_trans_skew_df}
# List of group names
group_names <- c("mean", "se", "worst")

# Create an empty list to store the skewness values
skewness_list <- list()

# Loop through each group
for (group_name in group_names) {
  # Subset the columns based on group name
  group_columns <- grep(group_name, names(df_var), value = TRUE)
  
  # Loop through each column in the group
  for (col_name in group_columns) {
    skewness <- skewness(df_var[[col_name]])
    
    # Print skewness value
    cat("Skewness for", col_name, ":", skewness, "\n")
    
    # Store the skewness value in the list
    skewness_list[[col_name]] <- skewness
  }
}

# Convert the skewness list to a dataframe
skewness_df <- data.frame(Skewness = unlist(skewness_list))

# Print the skewness dataframe
print(skewness_df)
```

#### Rough PCA for kmeans - Replace w !!!!Trevors!!!!!

```{r}
options(scipen = 999)

# Convert the dataframe to a matrix
df_pca <- as.matrix(df_var)

# Perform PCA
df.pca <- prcomp(df_pca, center = TRUE, scale. = TRUE)

# Calculate percentage variance explained
var_explained <- round(df.pca$sdev^2 / sum(df.pca$sdev^2) * 100, 4)

# Plot Scree Plot
library(factoextra)
library(ggplot2)

fviz_eig(df.pca,
         main = "Scree Plot of Principal Components",
         xlab = "Principal Components",
         ylab = "Percent Variance Explained",
         barcolor = "grey", barfill = "grey",
         linecolor = "blue", addlabels = TRUE,
         ggtheme = theme_classic())


```

2 or 3 principal components. revisit & decide later.

#### K-means Clustering

```{r}
df_scale <- scale(df_var)

#Kmeans for 2 clusters
km2.res <- kmeans(df_scale, 2)

km2.plot <- fviz_cluster(km2.res, data = df_scale)

km2.plot <- km2.plot + ggtitle("K-Means Clustering Plot - 2 Clusters")

km2.plot


#Kmeans for 3 clusters
km3.res <- kmeans(df_scale, 3)

km3.plot <- fviz_cluster(km3.res, data = df_scale)

km3.plot <- km3.plot + ggtitle("K-Means Clustering Plot - 3 Clusters")

km3.plot
```

### Data Transformations - this may need to be removed

```{r}
# define the transformation or pre-processing 
df_trans <- preProcess(df_var, method = c("BoxCox", "center", "scale"))
#apply the transformation
df_boxcox <- predict(df_trans, df_var)
head(df_boxcox[,1:4])

#Rearranging variables for easiest continuity
df_original <- df_var
df <- df_boxcox
```

```{r post_trans_skew_df}
# List of group names
group_names <- c("mean", "se", "worst")

# Create an empty list to store the skewness values
skewness_list <- list()

# Loop through each group
for (group_name in group_names) {
  # Subset the columns based on group name
  group_columns <- grep(group_name, names(df), value = TRUE)
  
  # Loop through each column in the group
  for (col_name in group_columns) {
    skewness <- skewness(df[[col_name]])
    
    # Print skewness value
    cat("Skewness for", col_name, ":", skewness, "\n")
    
    # Store the skewness value in the list
    skewness_list[[col_name]] <- skewness
  }
}

# Convert the skewness list to a dataframe
skewness_df <- data.frame(Skewness = unlist(skewness_list))

# Print the skewness dataframe
print(skewness_df)
```


### script k-fold cross, this one works

```{r}
#y <- df_diag$diagnosis
df_var_lr <- df_var
df_var_lr$diagnosis <- df_diag$diagnosis
#df_diag$diagnosis = as.integer(factor(df_diag$diagnosis))-1
#df_var$diagnosis <-  y #df_diag$diagnosis
#y <- df_diag$diagnosis
# create folds for k-fold cross validation
x_value_lr <- df_var_lr[ , -which(names(df_var_lr) == "diagnosis")]
y_value_lr <- df_var_lr$diagnosis
df_preprocess_lr <- preProcess(x_value_lr, method = c('BoxCox', 'center', 'scale'))
df_predict_lr <- predict(df_preprocess_lr,x_value_lr)

# create folds and apply them
df_folds_lr <- createFolds(df_var_lr$diagnosis, returnTrain = TRUE)
crtl_df_lr <- trainControl(method = 'cv',
                        summaryFunction = twoClassSummary,
                        classProbs = TRUE,
                        savePredictions = TRUE,
                        index = df_folds_lr)
```


```{r}
# cv split for xgboost
df_diag$diagnosis = as.integer(factor(df_diag$diagnosis))-1
df_var$diagnosis <- df_diag$diagnosis

# create folds for k-fold cross validation
df_preprocess <- preProcess(df_var, method = c('BoxCox', 'center', 'scale'))
df_predict <- predict(df_preprocess,df_var)

# create folds and apply them
df_folds <- createFolds(df_var$diagnosis, returnTrain = TRUE)
crtl_df <- trainControl(method = 'cv',
                        summaryFunction = twoClassSummary,
                        classProbs = TRUE,
                        savePredictions = TRUE,
                        index = df_folds)

```

```{r}
# 1st will be LR cross validation
pen_grid <-  expand.grid(alpha = c(0, .4, .8, 1),
                        lambda = seq(.01, .2, length = 10))


set.seed(100)
log_reg_fit <- train(df_predict_lr,
                     y_value_lr,
                     method = 'glmnet',
                     tuneGrid = pen_grid,
                     metric = 'ROC',
                     trControl = crtl_df_lr)
                     #family = "binomial")
log_reg_fit
confusionMatrix(log_reg_fit, norm = 'none')
cv_lr_roc <- roc(response = log_reg_fit$pred$obs,
                 predictor = log_reg_fit$pred$M,
                 levels = rev(levels(log_reg_fit$pred$obs)))
```

```{r}
#Universal XGBoost Set Up

#parameters
param <- list(
  "objective"           = "binary:logistic",
  "eval_metric"         = "auc",
  "eta"                 = 0.01,
  "max_depth"           = 6,
  "subsample"           = 0.8,
  "colsample_bytree"    = 0.8,
  "min_child_weight"    = 1,
  "gamma"               = 0
)

xgb.nround <- 1000
earlyStoppingRound <- 250
xgb.nfold <- 5
```

#Cross Validation XGBoost
```{r}
#matrix prep 
set.seed(0)
for (i in 1:xgb.nfold) {
  # Split the data into training and testing sets based on the fold
  cvtrain_indices <- unlist(df_folds[-i])
  cvtest_indices <- df_folds[[i]]
  cvtrain_data <- df_var[cvtrain_indices, ]
  cvtest_data <- df_var[cvtest_indices, ]}
  
# Create xgb.DMatrix for training and testing data
  cvtrain_data_matrix <- as.matrix(cvtrain_data[, -1])
  cvtrain_data_label <- as.numeric(cvtrain_data$diagnosis)
  cv_xgbtrain <- xgb.DMatrix(data = cvtrain_data_matrix, label = cvtrain_data_label)
  
  cvtest_data_matrix <- as.matrix(cvtest_data[, -1])
  cvtest_data_label <- as.numeric(cvtest_data$diagnosis)
  cv_xgbtest <- xgb.DMatrix(data = cvtest_data_matrix, label = cvtest_data_label)

# Train the xgboost model using xgb.cv
  cv_model_xgb_crossval <- xgb.cv(
    params = param,
    data = cv_xgbtrain,
    nrounds = xgb.nround,
    maximize = TRUE,
    nfold = xgb.nfold,
    prediction = TRUE,
    early_stopping_round = earlyStoppingRound, 
    verbose = 0
  )

# Extract the best iteration from the cross-validated model
  best_iteration <- cv_model_xgb_crossval$best_iteration

# Train the xgboost model using xgboost with the best iteration
  cv_model_xgb <- xgboost(
    params = param,
    data = cv_xgbtrain,
    nrounds = best_iteration,
    maximize = TRUE,
          verbose = 0 
    
  )
  
# Make predictions on the test set using the xgboost model
  cv_predictions <- predict(cv_model_xgb, newdata = cv_xgbtest)
```


```{r}
#Metrics
# Convert predicted labels and actual labels to factors with the same levels
cv_predicted_labels <- factor(ifelse(cv_predictions > 0.5, 1, 0), levels = c(0, 1))
cvtest_data$diagnosis <- factor(cvtest_data$diagnosis, levels = c(0, 1))

# Create a confusion matrix
cv_CM <- confusionMatrix(data = cv_predicted_labels, reference = cvtest_data$diagnosis)
cv_CM

# Convert predicted probabilities and actual labels to vectors
cv_predicted_probs <- as.numeric(cv_predictions)
cv_actual_labels <- as.numeric(as.character(cvtest_data$diagnosis))

# Calculate the ROC curve
cv_roc_results_xgb <- roc(cv_actual_labels, cv_predicted_probs)

# Get the sensitivity value (True Positive Rate)
cv_sens <- cv_CM$byClass["Sensitivity"]
cv_sens
cv_spec <- cv_CM$byClass["Specificity"]
cv_spec

```


#### 70/30 Split
```{r}
#randomly split the data 70/30 LR
trainingRows <- createDataPartition(df_var_lr$diagnosis, p = 0.7, list = FALSE)
df_train_lr <- df_var_lr[trainingRows, ]
df_test_lr <- df_var_lr[-trainingRows, ]
train_imp_lr <- preProcess(df_train_lr, method = c("BoxCox", 'center', 'scale'))
trainpre_lr <- predict(train_imp_lr, df_train_lr)
testpre_lr <- predict(train_imp_lr, df_test_lr)
X_train_lr <- trainpre_lr[, -which(names(trainpre_lr) == "diagnosis")]
y_train_lr <- trainpre_lr$diagnosis
X_test_lr <- testpre_lr[, -which(names(testpre_lr) == "diagnosis")]
y_test_lr <- testpre_lr$diagnosis
```

```{r}
# 70/30 split for xgboost
df_diag$diagnosis = as.integer(factor(df_diag$diagnosis))-1
trainingRows <- createDataPartition(df_diag$diagnosis, p = 0.7, list = FALSE)
df_train <- df_diag[trainingRows, ]
df_test <- df_diag[-trainingRows, ]
X_train <- df_train[, -which(names(df_train) == "diagnosis")]
y_train <- df_train$diagnosis
X_test <- df_test[, -which(names(df_test) == "diagnosis")]
y_test <- df_test$diagnosis

```

```{r}
# create model 3 random 70/30 split for LR
set.seed(100)
pen_grid_2 <-  expand.grid(alpha = c(0, .4, .8, 1),
                        lambda = seq(.01, .2, length = 10))
log_reg_fit_3 <- train(x =X_train_lr, y= y_train_lr,
                     method = 'glmnet',
                     metric = 'ROC',
                     tuneGrid = pen_grid_2,
                     trControl = trainControl(classProbs = TRUE, summaryFunction = twoClassSummary)) #, type.measure = "class"))

log_reg_fit_3
# obtain predictions 
predictions_lr_3 <- predict(log_reg_fit_3, newdata = X_test_lr)
# build confusion matrix
# set y_test as a factor
y_test_factor_3 <- as.factor(y_test_lr)
confusionMatrix(data = predictions_lr_3, reference = y_test_factor_3)

# calculate the ROC scores
y_test_num_3 <- as.numeric(y_test_factor_3)
roc_results_lr_3 <- roc(response = predictions_lr_3, predictor = y_test_num_3)
roc_results_lr_3
```

```{r}
set.seed(0)
#Random Split XGBoost

#matrix prep 
rs_xgbtrain <- xgb.DMatrix(data = as.matrix(X_train), label = y_train)
rs_xgbtest <- xgb.DMatrix(data = as.matrix(X_test), label = y_test)

rs_model_xgb_crossval <- xgb.cv(
  params = param, 
  data = rs_xgbtrain,
  nrounds = xgb.nround,
  maximize = TRUE,
  nfold = xgb.nfold, 
  prediction = TRUE,
  early_stopping_round=earlyStoppingRound, 
  verbose = 0
)

#Model
rs_model_xgb <- xgboost(
  params = param, 
  data = rs_xgbtrain,
  nrounds = xgb.nround,
  maximize = TRUE, 
  early_stopping_round=earlyStoppingRound, 
  verbose = 0
)

#Predict
X_test$predicted <- round(predict(object = rs_model_xgb ,newdata = rs_xgbtest),0)
```


```{r}
#Metrics
RS_xgb_AUC <- auc(y_test, X_test$predicted)
RS_xgb_AUC

rs_CM <- confusionMatrix(factor(X_test$predicted),factor(y_test))
rs_CM 

# Get the sensitivity value (True Positive Rate)
rs_sens <- rs_CM$byClass["Sensitivity"]
rs_sens
rs_spec <- rs_CM$byClass["Specificity"]
rs_spec

# calculate the ROC scores
rs_roc_results_xgb <- roc(response = X_test$predicted, predictor = y_test_num_3)
rs_roc_results_xgb
```

```{r}
# perform a random oversampling on the dataset
library(ROSE)

# Splitting the data into training and testing sets
trainingRows_2 <- createDataPartition(df_var_lr$diagnosis, p = 0.7, list = FALSE)
df_train_2_lr <- df_var_lr[trainingRows_2, ]
df_test_2_lr <- df_var_lr[-trainingRows_2, ]
# scale and transform the data 
trainimp_lr_2 <- preProcess(df_train_2_lr, method = c("BoxCox", 'center', 'scale'))
train_pre_lr_2 <- predict(trainimp_lr_2, df_train_2_lr)
test_pre_lr_2 <- predict(trainimp_lr_2, df_test_2_lr)

# Separate predictors and target variables in the training set
#X_train_2 <- df_train_2[, -which(names(df_train_2) == "diagnosis")]
#y_train_2 <- df_train_2$diagnosis

# Perform oversampling on the training set
oversampled_data_lr <- ovun.sample(diagnosis ~ ., data = train_pre_lr_2, method = "over")

# Extract the oversampled predictors and target variables
X_train_oversampled_lr <- oversampled_data_lr$data[, -which(names(oversampled_data_lr$data) == "diagnosis")]
y_train_oversampled_lr <- oversampled_data_lr$data$diagnosis

# Separate predictors and target variables in the testing set
X_test_oversampled_lr <- test_pre_lr_2[, -which(names(test_pre_lr_2) == "diagnosis")]
y_test_oversampled_lr <- test_pre_lr_2$diagnosis
```

```{r}
#70/30 oversampled split
# Splitting the data into training and testing sets
trainingRows_2 <- createDataPartition(df_diag$diagnosis, p = 0.7, list = FALSE)
df_train_2 <- df_diag[trainingRows_2, ]
df_test_2 <- df_diag[-trainingRows_2, ]

# Separate predictors and target variables in the training set
X_train_2 <- df_train_2[, -which(names(df_train_2) == "diagnosis")]
y_train_2 <- df_train_2$diagnosis

# Perform oversampling on the training set
oversampled_data <- ovun.sample(diagnosis ~ ., data = df_train_2, method = "over")

# Extract the oversampled predictors and target variables
X_train_oversampled <- oversampled_data$data[, -which(names(oversampled_data$data) == "diagnosis")]
y_train_oversampled <- oversampled_data$data$diagnosis

# Separate predictors and target variables in the testing set
X_test_oversampled <- df_test_2[, -which(names(df_test_2) == "diagnosis")]
y_test_oversampled <- df_test_2$diagnosis
```


```{r}
# create model 2 using the random over sampler technique on a 70/30 split
pen_grid_2 <-  expand.grid(alpha = c(0, .4, .8, 1),
                        lambda = seq(.01, .2, length = 10))
set.seed(100)
log_reg_fit_2 <- train(x =X_train_oversampled_lr, y= y_train_oversampled_lr,
                     method = 'glmnet',
                     metric = 'ROC',
                     tuneGrid = pen_grid_2,
                     trControl = trainControl(classProbs = TRUE, summaryFunction = twoClassSummary))

log_reg_fit_2
# obtain predictions 
predictions_lr <- predict(log_reg_fit_2, newdata = X_test_oversampled_lr)
# build confusion matrix
# set y_test as a facotr
y_test_factor_lr <- as.factor(y_test_oversampled_lr)
confusionMatrix(data = predictions_lr, reference = y_test_factor_lr)

# calculate the ROC scores
y_test_num_lr <- as.numeric(y_test_factor_lr)
roc_results_lr <- roc(response = predictions_lr, predictor = y_test_num_lr)
```

```{r}
set.seed(0)
#Oversampled XGBoost

#matrix prep 
os_xgbtrain <- xgb.DMatrix(data = as.matrix(X_train_oversampled), label = y_train_oversampled)
os_xgbtest <- xgb.DMatrix(data = as.matrix(X_test_oversampled), label = y_test_oversampled)

os_model_xgb_crossval <- xgb.cv(
  params = param, 
  data = os_xgbtrain,
  nrounds = xgb.nround,
  maximize = TRUE,
  nfold = xgb.nfold, 
  prediction = TRUE,
  early_stopping_round=earlyStoppingRound, 
  verbose = 0
)

#Model
os_model_xgb <- xgboost(
  params = param, 
  data = os_xgbtrain,
  nrounds = xgb.nround,
  maximize = TRUE,
  early_stopping_round=earlyStoppingRound, 
  verbose = 0 
)

#Predict
X_test$predicted <- round(predict(object = os_model_xgb ,newdata = os_xgbtest),0)
```

```{r}
#Metrics
os_xgb_AUC <- auc(y_test, X_test$predicted)
os_xgb_AUC


os_CM <- confusionMatrix(factor(X_test$predicted),factor(y_test_oversampled))
os_CM

# Get the sensitivity value (True Positive Rate)
os_sens <- os_CM$byClass["Sensitivity"]
os_sens
os_spec <- os_CM$byClass["Specificity"]
os_spec

# calculate the ROC scores
os_roc_results_xgb <- roc(response = X_test$predicted, predictor = y_test_oversampled)
os_roc_results_xgb
```

```{r}
# plot the ROC curve for each of the 3 Lr models
plot(cv_lr_roc, col = "red", main = "Logistic Regression ROC Curve", xlab = "False Positive Rate (Specifity)", ylab = "True Positive Rate (Sensitivity)", print.auc = FALSE, auc.polygon = TRUE, auc.polygon.col = "lightgray")
lines(roc_results_lr, col = "blue", print.auc = FALSE, auc.polygon = TRUE, auc.polygon.col = "lightgray")
lines(roc_results_lr_3, col = "green", print.auc = FALSE, auc.polygon = TRUE, auc.polygon.col = "lightgray")


# Add a legend
legend("bottomright", legend = c("Log Reg w/ CV", "Log Reg w/ Oversampler" , "Log Reg w/ Random Split"), col = c("red", "blue", "green"), lty = 1)
```

```{r}
# plot the ROC curve for each of the 3 XBoost models
plot(cv_roc_results_xgb, col = "red", main = "XG Boost ROC Curve", xlab = "False Positive Rate (Specifity)", ylab = "True Positive Rate (Sensitivity)", print.auc = FALSE, auc.polygon = TRUE, auc.polygon.col = "lightgray")
lines(rs_roc_results_xgb, col = "blue", print.auc = FALSE, auc.polygon = TRUE, auc.polygon.col = "lightgray")
lines(os_roc_results_xgb, col = "green", print.auc = FALSE, auc.polygon = TRUE, auc.polygon.col = "lightgray")
legend("bottomright", legend = c("CV XGB", "RS XGB", "OS XGB"), col = c("red", "blue", "green"), lty = 1, lwd = 2)
```

```{r}
lr_1 <- varImp(log_reg_fit, scale = FALSE)
lr_2 <- varImp(log_reg_fit_2, scale = FALSE)
lr_3 <- varImp(log_reg_fit_3, scale = FALSE)
plot(lr_1)
plot(lr_2)
plot(lr_3)
```

```{r}
calculate_metrics <- function(model, train_data, train_labels, test_data, test_labels) {
  # Train set predictions
  train_predictions <- predict(model, newdata = train_data)
  
  # Test set predictions
  test_predictions <- predict(model, newdata = test_data)
  
  # Train set metrics
  train_accuracy <- mean(train_predictions == train_labels)
  train_sensitivity <- sum(train_predictions[train_labels == "B"] == "B") / sum(train_labels == "B")
  train_specificity <- sum(train_predictions[train_labels == "M"] == "M") / sum(train_labels == "M")
  
  # Test set metrics
  test_accuracy <- mean(test_predictions == test_labels)
  test_sensitivity <- sum(test_predictions[test_labels == "B"] == "B") / sum(test_labels == "B")
  test_specificity <- sum(test_predictions[test_labels == "M"] == "M") / sum(test_labels == "M")
  
  # Return the metrics as a named list
  metrics <- list(
    train_accuracy = train_accuracy,
    train_sensitivity = train_sensitivity,
    train_specificity = train_specificity,
    test_accuracy = test_accuracy,
    test_sensitivity = test_sensitivity,
    test_specificity = test_specificity
  )
  
  return(metrics)
}

# Use function to get metrics for training and testing set of 70/30 oversampled
metrics <- calculate_metrics(log_reg_fit_2, X_train_oversampled_lr, y_train_oversampled_lr, X_test_oversampled_lr, y_test_oversampled_lr)

# Extracts the metrics from the metrics list
train_accuracy_2 <- metrics$train_accuracy
train_sensitivity_2 <- metrics$train_sensitivity
train_specificity_2 <- metrics$train_specificity
test_accuracy_2 <- metrics$test_accuracy
test_sensitivity_2 <- metrics$test_sensitivity
test_specificity_2 <- metrics$test_specificity
```

```{r}
print("Oversampled with 70/30 Split")
cat("Training Accuracy", train_accuracy_2)
print("")
cat("Testing Accuracy", test_accuracy_2)
print("")
cat("Train Sensitivity", train_sensitivity_2)
print("")
cat("Test Sensitivity", test_sensitivity_2)
print("")
cat("Train Specifity", train_specificity_2)
print("")
cat("Test Specifity", test_specificity_2)
```

```{r}
# Use function to get metrics for training and testing set of 70/30 oversampled
metrics_2 <- calculate_metrics(log_reg_fit_3, X_train_lr, y_train_lr, X_test_lr, y_test_lr)

# Extracts the metrics from the metrics list
train_accuracy_3 <- metrics$train_accuracy
train_sensitivity_3 <- metrics$train_sensitivity
train_specificity_3 <- metrics$train_specificity
test_accuracy_3 <- metrics$test_accuracy
test_sensitivity_3 <- metrics$test_sensitivity
test_specificity_3 <- metrics$test_specificity

print("Oversampled with 70/30 Split")
cat("Training Accuracy", train_accuracy_3)
print("")
cat("Testing Accuracy", test_accuracy_3)
print("")
cat("Train Sensitivity", train_sensitivity_3)
print("")
cat("Test Sensitivity", test_sensitivity_3)
print("")
cat("Train Specifity", train_specificity_3)
print("")
cat("Test Specifity", test_specificity_3)
```