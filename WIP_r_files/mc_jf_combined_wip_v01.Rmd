---
title: "Project_JF"
author: "Justin Farnan"
date: "2023-06-08"
output: html_document
---

```{r}
# load in dataset
library(ggplot2)
library(tidyr)
library(caret)
library(stats)
library(factoextra)
library(e1071)
df <- read.csv('data.csv')
# remove the x and Id from the end of the data set since it is all null values
# "x" column is an error from csv column spacing, and can be removed entirely. 
df$X <- NULL
df$id <- NULL
df_diag <- df
```


##### Summary Stats, Dimensions & NA values
```{r}
# get descriptive statistics for the data set
summary(df)
```
```{r}
# data set dimensions
cat("Dimensions of dataset:", dim(df))
```

```{r}
# NA values
df_na_counts <- sum(is.na(df))
cat("NA Sum:", df_na_counts)
```


#### Distribution of Outcomes

```{r}
# Calculate percentages
percentage_M <- sum(df$diagnosis == "M") / nrow(df) * 100
percentage_B <- sum(df$diagnosis == "B") / nrow(df) * 100

# Print the percentages
cat("Percentage of Malignant diagnosis:", percentage_M,"%\n")
cat("Percentage of Benign diagnosis:", percentage_B,"%\n")
```

#### Exploring Possible Near Zero Variances
```{r}
degeneratecols <- nearZeroVar(df)
degeneratecols
```
There appears to be no degenerate variables. 

#### Splitting into groups to allow for easier visualizations
```{r}
# Identify the columns containing "mean"
mean_columns <- grep("mean", names(df), value = TRUE)

# Identify the columns containing "se"
se_columns <- grep("se", names(df), value = TRUE)

# Identify the columns containing "worst"
worst_columns <- grep("worst", names(df), value = TRUE)

# Split the dataframe into three groups based on the keywords
df_mean <- df[, mean_columns]
df_se <- df[, se_columns]
df_worst <- df[, worst_columns]
```

#### df_var data frame for visualizing just predictors
```{r}
remove <- c("diagnosis")
df_var <- df[, !(colnames(df) %in% remove)]
```

#### Histograms of all predictor variables
```{r}
par(mfrow = c(5, 2))
par(mar = c(2.5, 2, 1, 1))
par(cex.main = 1)

for (i in 1:ncol(df_mean)) {
  hist(df_var[, i], xlab = names(df_var)[i], main = paste(names(df_var)[i]))
}

for (i in 1:ncol(df_se)) {
  hist(df_var[, i], xlab = names(df_var)[i], main = paste(names(df_var)[i]))
}

for (i in 1:ncol(df_worst)) {
  hist(df_var[, i], xlab = names(df_var)[i], main = paste(names(df_var)[i]))
}

```



```{r}
# build a boxplot to identity the outliers that were produced in the summary statistics
# split the data into groups of 5 and create 6 graphs for the plots since there are 30 features and 1 target
# data will be split into variables bx_1, bx_2, bx_3, bx_4, bx_5 for boxplot purposes
colors <- c('red', 'blue')
bx_1 <- df[, 2:6]
bx_2 <- df[, 7:12]
bx_3 <- df[, 13:18]
bx_4 <- df[, 19:24]
bx_5 <- df[, 25:31]
```
All of these predictors have outliers however the predictor with the most are texture_mean, area_mean ad smoothness mean. 





For bx_2 all of these also have outliers and I would say they all have some pretty significant outliers. 


```{r}
# Create a data frame for plotting
plot_data_1 <- data.frame(
  value = c(bx_1$radius_mean, bx_1$texture_mean, bx_1$perimeter_mean, bx_1$smoothness_mean, bx_1$area_mean),
  diagnosis = rep(df$diagnosis, 5),
  feature = rep(c("Radius Mean", "Texture Mean", "Perimeter Mean", "Smoothness Mean", "Area Mean"), each = nrow(df))
)

# Create the boxplot using ggplot2
ggplot(plot_data_1, aes(x = diagnosis, y = value, fill = diagnosis)) +
  geom_boxplot() +
  facet_wrap(~ feature, scales = "free_y", nrow = 1) +
  scale_fill_manual(values = colors, labels = c('Bengin', 'Malignant')) +
  xlab("Diagnosis") +
  ylab("Value") +
  theme(legend.position = "top")
```

```{r}
# Create a data frame for plotting
plot_data_2 <- data.frame(
  value = c(bx_2$fractal_dimension_mean,bx_2$symmetry_mean, bx_2$radius_se, bx_2$concave.points_mean, bx_2$concavity_mean,bx_2$compactness_mean  ),
  diagnosis = rep(df$diagnosis, 6),
  feature = rep(c("FractalMean", "Sym Mean", "Radius Se", "Con Mean", "Concavity Mean", 'Compact Mean'), each = nrow(df))
)

# Create the boxplot using ggplot2
ggplot(plot_data_2, aes(x = diagnosis, y = value, fill = diagnosis)) +
  geom_boxplot() +
  facet_wrap(~ feature, scales = "free_y", nrow = 1) +
  scale_fill_manual(values = colors, labels = c('Bengin', 'Malignant')) +
  xlab("Diagnosis") +
  ylab("Value") +
  theme(legend.position = "top")
```
```{r}
# Create a data frame for plotting
plot_data_3 <- data.frame(
  value = c(bx_3$texture_se,bx_3$perimeter_se, bx_3$area_se, bx_3$smoothness_se, bx_3$compactness_se, bx_3$concavity_se),
  diagnosis = rep(df$diagnosis, 6),
  feature = rep(c("Texture Se", "Perimeter Se", "Area Se", "Smoothness Se", "Compact Se", 'Concave Se'), each = nrow(df))
)

# Create the boxplot using ggplot2
ggplot(plot_data_3, aes(x = diagnosis, y = value, fill = diagnosis)) +
  geom_boxplot() +
  facet_wrap(~ feature, scales = "free_y", nrow = 1) +
  scale_fill_manual(values = colors, labels = c('Bengin', 'Malignant')) +
  xlab("Diagnosis") +
  ylab("Value") +
  theme(legend.position = "top")

```
```{r}

# Create a data frame for plotting
plot_data_4 <- data.frame(
  value = c(bx_4$concave.points_se,bx_4$symmetry_se, bx_4$fractal_dimension_se, bx_4$radius_worst, bx_4$texture_worst, bx_4$perimeter_worst),
  diagnosis = rep(df$diagnosis, 6),
  feature = rep(c("Concave Se", "Symmetry Se", "Fractal Se", "Radius Worst", "Texture Worst", 'Perimeter Worst'), each = nrow(df))
)

# Create the boxplot 
ggplot(plot_data_4, aes(x = diagnosis, y = value, fill = diagnosis)) +
  geom_boxplot() +
  facet_wrap(~ feature, scales = "free_y", nrow = 1) +
  scale_fill_manual(values = colors, labels = c('Bengin', 'Malignant')) +
  xlab("Diagnosis") +
  ylab("Value") +
  theme(legend.position = "top")

```

```{r}
# Create a data frame for plotting
plot_data_5 <- data.frame(
  value = c(bx_5$area_worst ,bx_5$smoothness_worst, bx_5$compactness_worst, bx_5$concave.points_worst, bx_5$symmetry_worst, bx_5$fractal_dimension_worst),
  diagnosis = rep(df$diagnosis, 6),
  feature = rep(c("Area Worst", "Smoothness Worst", "Compact Worst", "Concave Worst", "symmetry Worst", 'Fractal Worst'), each = nrow(df))
)

# Create the boxplot 
ggplot(plot_data_5, aes(x = diagnosis, y = value, fill = diagnosis)) +
  geom_boxplot() +
  facet_wrap(~ feature, scales = "free_y", nrow = 1) +
  scale_fill_manual(values = colors, labels = c('Bengin', 'Malignant')) +
  xlab("Diagnosis") +
  ylab("Value") +
  theme(legend.position = "top")
```

#### Skew
```{r pre_trans_skew}
# List of group names
group_names <- c("mean", "se", "worst")

# Set the main title font size
par(cex.main = 0.8)

# Loop through each group
for (group_name in group_names) {
  # Subset the columns based on group name
  group_columns <- grep(group_name, names(df_var), value = TRUE)

  # Calculate skewness and create plots for each column in the group
  num_plots <- length(group_columns)
  num_rows <- ceiling(num_plots / 5)
  num_cols <- min(num_plots, 5)
  
  # Set the plotting layout for the current group
  par(mfrow = c(num_rows, num_cols))
  
  # Loop through each column in the group
  for (i in 1:num_plots) {
    col_name <- group_columns[i]
    skewness <- skewness(df_var[[col_name]])
    
    # Create a histogram to visualize skewness
    hist(df_var[[col_name]], main = col_name,
         xlab = "Values", ylab = "Frequency",
         col = "gray", border = "black")
    
    # Add skewness value to the plot
    mtext(paste("Skew:", round(skewness, 2)), side = 3, line = -.25, cex = 0.5, font = 1)
  }
  
  # Reset the plotting layout
  par(mfrow = c(1, 1))
}
```

#### Skew in dataframe format for easy reference
```{r pre_trans_skew_df}
# List of group names
group_names <- c("mean", "se", "worst")

# Create an empty list to store the skewness values
skewness_list <- list()

# Loop through each group
for (group_name in group_names) {
  # Subset the columns based on group name
  group_columns <- grep(group_name, names(df_var), value = TRUE)
  
  # Loop through each column in the group
  for (col_name in group_columns) {
    skewness <- skewness(df_var[[col_name]])
    
    # Print skewness value
    cat("Skewness for", col_name, ":", skewness, "\n")
    
    # Store the skewness value in the list
    skewness_list[[col_name]] <- skewness
  }
}

# Convert the skewness list to a dataframe
skewness_df <- data.frame(Skewness = unlist(skewness_list))

# Print the skewness dataframe
print(skewness_df)
```

#### Rough PCA for kmeans - Replace w !!!!Trevors!!!!!
```{r}
options(scipen = 999)

# Convert the dataframe to a matrix
df_pca <- as.matrix(df_var)

# Perform PCA
df.pca <- prcomp(df_pca, center = TRUE, scale. = TRUE)

# Calculate percentage variance explained
var_explained <- round(df.pca$sdev^2 / sum(df.pca$sdev^2) * 100, 4)

# Plot Scree Plot
library(factoextra)
library(ggplot2)

fviz_eig(df.pca,
         main = "Scree Plot of Principal Components",
         xlab = "Principal Components",
         ylab = "Percent Variance Explained",
         barcolor = "grey", barfill = "grey",
         linecolor = "blue", addlabels = TRUE,
         ggtheme = theme_classic())


```

2 or 3 principal components. revisit & decide later.

#### K-means Clustering - !!! This needs class labels, similar to deseq!!
```{r}
df_scale <- scale(df_var)

#Kmeans for 2 clusters
km2.res <- kmeans(df_scale, 2)

km2.plot <- fviz_cluster(km2.res, data = df_scale)

km2.plot <- km2.plot + ggtitle("K-Means Clustering Plot - 2 Clusters")

km2.plot

#Kmeans for 3 clusters
km3.res <- kmeans(df_scale, 3)

km3.plot <- fviz_cluster(km3.res, data = df_scale)

km3.plot <- km3.plot + ggtitle("K-Means Clustering Plot - 3 Clusters")

km3.plot
```

### Data Transformations - this may need to be removed
```{r}
# define the transformation or pre-processing 
df_trans <- preProcess(df_var, method = c("BoxCox", "center", "scale"))
#apply the transformation
df_boxcox <- predict(df_trans, df_var)
head(df_boxcox[,1:4])

#Rearranging variables for easiest continuity
df_original <- df_var
df <- df_boxcox
```

```{r post_trans_skew_df}
# List of group names
group_names <- c("mean", "se", "worst")

# Create an empty list to store the skewness values
skewness_list <- list()

# Loop through each group
for (group_name in group_names) {
  # Subset the columns based on group name
  group_columns <- grep(group_name, names(df), value = TRUE)
  
  # Loop through each column in the group
  for (col_name in group_columns) {
    skewness <- skewness(df[[col_name]])
    
    # Print skewness value
    cat("Skewness for", col_name, ":", skewness, "\n")
    
    # Store the skewness value in the list
    skewness_list[[col_name]] <- skewness
  }
}

# Convert the skewness list to a dataframe
skewness_df <- data.frame(Skewness = unlist(skewness_list))

# Print the skewness dataframe
print(skewness_df)
```
### rmd kfold cross- has errors
```{r eval=FALSE, include=FALSE}
table(df$diagnosis)
# given that the data is imbalanced we have two option for splitting the data, either kfold cross validation or a simple train test split with an over or under sampler applied
# K fold cross validation dataset build
#degen_cols<- nearZeroVar(df) <- shown to be zero earlier
correlation <- df[ , -which(names(df) == "diagnosis")]
df_cor <- cor(correlation[sapply(correlation,is.numeric)])
#bio_cuttoff <- findCorrelation(df, cutoff = .75)
```

rmd kfold cross- has errors
```{r eval=FALSE, include=FALSE}
# create folds for k-fold cross validation Logsitic regression
df_preprocess <- preProcess(df, method = c('BoxCox', 'center', 'scale'))
df_predict <- predict(df_preprocess,df)
```

```{r eval=FALSE, include=FALSE}
# create folds and apply them
df_folds <- createFolds(df$diagnosis, returnTrain = TRUE)
crtl_df <- trainControl(method = 'cv',
                         summaryFunction = twoClassSummary,
                         classProbs = TRUE,
                         savePredictions = TRUE,
                         index = df_folds)
```

### script k-fold cross, this one works
```{r}
df_var$diagnosis <- df_diag$diagnosis
# create folds for k-fold cross validation Logsitic regresson
df_preprocess <- preProcess(df_var, method = c('BoxCox', 'center', 'scale'))
df_predict <- predict(df_preprocess,df_var)

# create folds for k-fold cross validation Logsitic regresson
df_preprocess <- preProcess(df_var, method = c('BoxCox', 'center', 'scale'))
df_predict <- predict(df_preprocess,df_var)
# create folds and apply them
df_folds <- createFolds(df_var$diagnosis, returnTrain = TRUE)
crtl_df <- trainControl(method = 'cv',
                        summaryFunction = twoClassSummary,
                        classProbs = TRUE,
                        savePredictions = TRUE,
                        index = df_folds)
```

```{r}
#randomly split the data 70/30
trainingRows <- createDataPartition(df$diagnosis, p = 0.7, list = FALSE)
df_train <- df[trainingRows, ]
df_test <- df[-trainingRows, ]
X_train <- df_train[, -which(names(train) == "diagnosis")]
y_train <- df_train$diagnosis
X_test <- df_test[, -which(names(df_test) == "diagnosis")]
y_test <- df_test$diagnosis
```

```{r}
# perform a random oversampling on the dataset
library(ROSE)

# Splitting the data into training and testing sets
trainingRows_2 <- createDataPartition(df$diagnosis, p = 0.7, list = FALSE)
df_train_2 <- df[trainingRows_2, ]
df_test_2 <- df[-trainingRows_2, ]

# Separate predictors and target variables in the training set
X_train_2 <- df_train_2[, -which(names(df_train_2) == "diagnosis")]
y_train_2 <- df_train_2$diagnosis

# Perform oversampling on the training set
oversampled_data <- ovun.sample(diagnosis ~ ., data = df_train_2, method = "over")

# Extract the oversampled predictors and target variables
X_train_oversampled <- oversampled_data$data[, -which(names(oversampled_data$data) == "diagnosis")]
y_train_oversampled <- oversampled_data$data$diagnosis

# Separate predictors and target variables in the testing set
X_test_oversampled <- df_test_2[, -which(names(df_test_2) == "diagnosis")]
y_test_oversampled <- df_test_2$diagnosis
```
